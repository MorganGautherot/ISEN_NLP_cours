{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercice_7_solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wXKdTbCG6VT"
      },
      "source": [
        "# Implémentation du CBOW avec Keras\n",
        "\n",
        "Dans ce notebook, vous allez implémenter et entraîner le CBOW avec Keras.\n",
        "\n",
        "L'implémentation va se diviser en quatre parties : \n",
        "- Construire le vocabulaire du corpus\n",
        "- Construire un générateur de CBOW (contexte, cible)\n",
        "- Construire l'architecture du modèle CBOW\n",
        "- Entraîner le modèle\n",
        "- Obtenir les words embeddings (options 1, 2 et 3)\n",
        "- Visualiser les mots similaires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDSVBE4CX87T"
      },
      "source": [
        "# Importation de fichiers\n",
        "\n",
        "Avant de commencer veuillez importer les fichiers du TP via le lien ci-dessous :\n",
        "\n",
        "https://drive.google.com/drive/folders/1S8u7-8dnsRCawJ8tAg1VzB4TvTdcbN7Q?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU2FBi0JX_x1",
        "outputId": "b8afbc16-2a4e-40ae-ada0-0c2adb0ea215"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQt_wqVWYYxt"
      },
      "source": [
        "# Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlNMb3DeYYSd",
        "outputId": "8aa0784b-1837-4941-fb83-d1e8a8a2a0c2"
      },
      "source": [
        "# Import Python libraries and helper functions (in utils2) \n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from gdrive.MyDrive.TP_6.utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
        "\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0sazuLrYeWS"
      },
      "source": [
        "# Importations des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qc6LafYUpI",
        "outputId": "b8aef26f-1eae-464b-f13a-55a7f1ca94da"
      },
      "source": [
        "# Load, tokenize and process the data\n",
        "import re                                                           #  Load the Regex-modul\n",
        "with open('gdrive/MyDrive/TP_6/shakespeare.txt') as f:\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens: 60933 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKUCr7GpYoZ2"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfxAMjz2iQDX",
        "outputId": "4c6676fc-b111-4865-eb08-334f3a3742d1"
      },
      "source": [
        "\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "# build vocabulary of unique words\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 5772\n",
            "Vocabulary Sample: [('the', 1), ('and', 2), ('i', 3), ('to', 4), ('of', 5), ('my', 6), ('that', 7), ('in', 8), ('you', 9), ('a', 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YErFmZ0KYrnE"
      },
      "source": [
        "# Création du dataset d'entraînement\n",
        "\n",
        "Extraire les mots de contextes et les mots centrés. \n",
        "Créer un dataset avec ces mots "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwuzonfCHHv1"
      },
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([words[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "\n",
        "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
        "            y = np_utils.to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)\n",
        "            \n",
        "            \n",
        "# Test this out for some samples\n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR_VFMg0Y9zm"
      },
      "source": [
        "# Initialisation du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pl7rPFgZNNc"
      },
      "source": [
        "Vous pouvez utiliser la fonction *Embedding* afin de transformer vos mots en vecteurs.\n",
        "\n",
        "Documentation de la fonction [ici](https://keras.io/api/layers/core_layers/embedding/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "4XdB41nHY8cs",
        "outputId": "36be8f51-348a-4923-f991-d63e24207859"
      },
      "source": [
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "# build CBOW architecture\n",
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# view model summary\n",
        "print(cbow.summary())\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 100)            577200    \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5772)              582972    \n",
            "=================================================================\n",
            "Total params: 1,160,172\n",
            "Trainable params: 1,160,172\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"405pt\" viewBox=\"0.00 0.00 252.00 304.00\" width=\"336pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 248,-300 248,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140437873390536 -->\n<g class=\"node\" id=\"node1\">\n<title>140437873390536</title>\n<polygon fill=\"none\" points=\"12.5,-249.5 12.5,-295.5 231.5,-295.5 231.5,-249.5 12.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.5\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"92.5,-249.5 92.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"92.5,-272.5 150.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"150.5,-249.5 150.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-280.3\">[(None, 4)]</text>\n<polyline fill=\"none\" points=\"150.5,-272.5 231.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-257.3\">[(None, 4)]</text>\n</g>\n<!-- 140439593001760 -->\n<g class=\"node\" id=\"node2\">\n<title>140439593001760</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 244,-212.5 244,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-185.8\">Embedding</text>\n<polyline fill=\"none\" points=\"84,-166.5 84,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"84,-189.5 142,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"142,-166.5 142,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-197.3\">(None, 4)</text>\n<polyline fill=\"none\" points=\"142,-189.5 244,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-174.3\">(None, 4, 100)</text>\n</g>\n<!-- 140437873390536&#45;&gt;140439593001760 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140437873390536-&gt;140439593001760</title>\n<path d=\"M122,-249.3799C122,-241.1745 122,-231.7679 122,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-222.784 122,-212.784 118.5001,-222.784 125.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140437873391208 -->\n<g class=\"node\" id=\"node3\">\n<title>140437873391208</title>\n<polygon fill=\"none\" points=\"10,-83.5 10,-129.5 234,-129.5 234,-83.5 10,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-102.8\">Lambda</text>\n<polyline fill=\"none\" points=\"74,-83.5 74,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"74,-106.5 132,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"132,-83.5 132,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-114.3\">(None, 4, 100)</text>\n<polyline fill=\"none\" points=\"132,-106.5 234,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-91.3\">(None, 100)</text>\n</g>\n<!-- 140439593001760&#45;&gt;140437873391208 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140439593001760-&gt;140437873391208</title>\n<path d=\"M122,-166.3799C122,-158.1745 122,-148.7679 122,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-139.784 122,-129.784 118.5001,-139.784 125.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140437843951968 -->\n<g class=\"node\" id=\"node4\">\n<title>140437843951968</title>\n<polygon fill=\"none\" points=\"19.5,-.5 19.5,-46.5 224.5,-46.5 224.5,-.5 19.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"45.5\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"71.5,-.5 71.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"100.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"71.5,-23.5 129.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"100.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"129.5,-.5 129.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-31.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"129.5,-23.5 224.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-8.3\">(None, 5772)</text>\n</g>\n<!-- 140437873391208&#45;&gt;140437843951968 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140437873391208-&gt;140437843951968</title>\n<path d=\"M122,-83.3799C122,-75.1745 122,-65.7679 122,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-56.784 122,-46.784 118.5001,-56.784 125.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEsqAPZvZhA0"
      },
      "source": [
        "# Entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZK3JjaBZD8N",
        "outputId": "d4f3e02f-1e6f-4efb-e263-c76c9ab76d8b"
      },
      "source": [
        "for epoch in range(1, 2):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 10000 (context, word) pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxRPR8ZpZrHf"
      },
      "source": [
        "# Extraction du word embeddings option 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivlIkVjOZ3wF"
      },
      "source": [
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai4UmzguZxJg"
      },
      "source": [
        "# Extraction du word embeddings option 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWGucDWgaIov"
      },
      "source": [
        "weights = cbow.get_weights()[1]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frTMsTTZZz2a"
      },
      "source": [
        "# Extraction du word embeddings option 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_S4Ulm9Zl61"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qby7ZRm_aQbb"
      },
      "source": [
        "# Afficher les mots similaires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7juH0FnHaSBx"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ijlu4S5aSqV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}