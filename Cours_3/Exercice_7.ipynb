{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercice_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wXKdTbCG6VT"
      },
      "source": [
        "# Implémentation du CBOW avec Keras\n",
        "\n",
        "Dans ce notebook, vous allez implémenter et entraîner le CBOW avec Keras.\n",
        "\n",
        "L'implémentation va se diviser en quatre parties : \n",
        "- Construire le vocabulaire du corpus\n",
        "- Construire un générateur de CBOW (contexte, cible)\n",
        "- Construire l'architecture du modèle CBOW\n",
        "- Entraîner le modèle\n",
        "- Obtenir les words embeddings (options 1, 2 et 3)\n",
        "- Visualiser les mots similaires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDSVBE4CX87T"
      },
      "source": [
        "# Importation de fichiers\n",
        "\n",
        "Avant de commencer veuillez importer les fichiers du TP via le lien ci-dessous :\n",
        "\n",
        "https://drive.google.com/drive/folders/1S8u7-8dnsRCawJ8tAg1VzB4TvTdcbN7Q?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU2FBi0JX_x1",
        "outputId": "b8afbc16-2a4e-40ae-ada0-0c2adb0ea215"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQt_wqVWYYxt"
      },
      "source": [
        "# Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlNMb3DeYYSd",
        "outputId": "8aa0784b-1837-4941-fb83-d1e8a8a2a0c2"
      },
      "source": [
        "# Import Python libraries and helper functions (in utils2) \n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from gdrive.MyDrive.TP_6.utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
        "\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0sazuLrYeWS"
      },
      "source": [
        "# Importations des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7qc6LafYUpI",
        "outputId": "b8aef26f-1eae-464b-f13a-55a7f1ca94da"
      },
      "source": [
        "# Load, tokenize and process the data\n",
        "import re                                                           #  Load the Regex-modul\n",
        "with open('gdrive/MyDrive/TP_6/shakespeare.txt') as f:\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens: 60933 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKUCr7GpYoZ2"
      },
      "source": [
        "# Pré-traitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YErFmZ0KYrnE"
      },
      "source": [
        "# Création du dataset d'entraînement\n",
        "\n",
        "Extraire les mots de contextes et les mots centrés. \n",
        "Créer un dataset avec ces mots "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR_VFMg0Y9zm"
      },
      "source": [
        "# Initialisation du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pl7rPFgZNNc"
      },
      "source": [
        "Vous pouvez utiliser la fonction *Embedding* afin de transformer vos mots en vecteurs.\n",
        "\n",
        "Documentation de la fonction [ici](https://keras.io/api/layers/core_layers/embedding/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEsqAPZvZhA0"
      },
      "source": [
        "# Entraînement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxRPR8ZpZrHf"
      },
      "source": [
        "# Extraction du word embeddings option 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai4UmzguZxJg"
      },
      "source": [
        "# Extraction du word embeddings option 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frTMsTTZZz2a"
      },
      "source": [
        "# Extraction du word embeddings option 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qby7ZRm_aQbb"
      },
      "source": [
        "# Afficher les mots similaires\n",
        "\n",
        "Afficher les mots les plus similaires de *jewel*, *saint*, *goddess* et *flower*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eg2MSTyeDof"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}